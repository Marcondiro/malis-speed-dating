##########################################
#                                        #
#           DATA PREPROCESSING           #
#                                        #
##########################################

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
import models
import numpy as np
import pandas as pd
import re


def load_dataset(dataset_file):
    """
    Reads the dataset from file, extracting the label column.

    
    Parameters
    ----------
    dataset_file: str
        The name of the file containing the dataset (.pkl)

    Returns
    -------
    X
        DataFrame containing the dataset samples
    y
        DataFrame containing the correspondent labels
    """

    # first, load the dataframe
    X = pd.read_pickle(dataset_file)

    # extract the label column, and remove it from the original dataframe
    y = X['match']
    X.drop('match', axis='columns', inplace=True)

    return X, y



def split_dataset(X, y, test=0.3, stratify=True):
    """
    An utility wrapper for train_test_split.

    Parameters
    ----------
    X: DataFrame
        The dataset samples
    y: DataFrame
        The corresponding labels
    test: float, optional (default is 0.3)
        The fraction of the dataset we want to keep as test set
    stratify: bool, optional (default is True)
        Whether we want the split to keep the same proportion between classes as the original dataset

    Returns
    -------
    X_tr, y_tr
        Samples and relative labels for the training set
    X_te, y_te
        Samples and relative labels for the test set

    """
    
    # split it into train and test set
    stratify = (y if stratify else None)
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test, stratify=stratify)

    return X_tr, y_tr, X_te, y_te



def grid_search(X, y, estimator, grid, k=5):

    """
    Perform grid search on the provided model/pipe.

    Parameters
    ----------
    X: DataFrame
        The dataset samples
    y: DataFrame
        The corresponding labels
    estimator:
        The sklearn model/pipe we want to apply
    grid: dictionary
        The grid of hyperparameters we want to test for our model 
    k: int, optional (default is 5)
        The number of folds for the k-fold validation approach

    Returns
    -------
    dict
        A subset of the GridSearchCV object.
        - best_model - the best estimator found in the grid search
        - best_params - parameters generating such an estimator
        - best_recall - the recall of such an estimator
        - best_balanced_accuracy - the balanced accuracy of such an estimator
        - best_f1 - the f1 of such an estimator
        - all_recall - the mean recall of all estimators generated by the grid
        - all_balanced_accuracy - the mean balanced accuracy of all estimators generated by the grid
        - all_f1 - the mean f1 of all estimators generated by the grid
    
    """

    # TODO add more? - CHOOSE 1 to get the best
    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
    metrics = ['balanced_accuracy', 'f1', 'recall']
    best_metric = 'recall'

    # create the GridSearchCV object
    grid_cv = GridSearchCV(estimator, param_grid=grid, cv=k, scoring=metrics, refit=best_metric)
    # run the grid search
    res = grid_cv.fit(X, y)

    out = {
        'best_params': res.best_params_,
        'best_model': res.best_estimator_,
        'best_index': res.best_index_,
        'best_recall': res.cv_results_['mean_test_recall'][res.best_index_],
        'best_balanced_accuracy': res.cv_results_['mean_test_balanced_accuracy'][res.best_index_],
        'best_f1': res.cv_results_['mean_test_f1'][res.best_index_],
        'all_recall': res.cv_results_['mean_test_recall'],
        'all_balanced_accuracy': res.cv_results_['mean_test_balanced_accuracy'],
        'all_f1': res.cv_results_['mean_test_f1']
    }

    return out


def interactions(X, drop=False):
    """
    Compute the interactions for the DataFrame X, using the following logic:
        col <- col_x * col_y

    Parameters
    ----------
    X: DataFrame
        The dataset samples
    drop: bool, optional (default is False)
        When True, we drop col_x and col_y, leaving just their interaction col

    Returns
    -------
    DataFrame
        The modified dataset

    """
    # add interaction x-y
    columns = list(X.columns)
    re_x = re.compile(".*_x.*")
    re_y = re.compile(".*_y.*")
    re_sub = re.compile("_x")
    pairs = [[c for c in columns if re_x.match(c)],
        [c for c in columns if re_y.match(c)], 
        [re.sub(re_sub, '', c) for c in columns if re_x.match(c)]
    ]

    X[pairs[2]] = np.multiply(X[pairs[0]], np.asarray(X[pairs[1]]))
    
    # drop the other columns, if necessary
    if drop:
        X.drop(pairs[0]+pairs[1], axis='columns', inplace=True)

    return X



##########################################
#                                        #
#               PARAMETERS               #
#                                        #
##########################################
test_size = 0.2         # the ratio of the dataset we want to use as test set
stratify = True         # Whether we want the split to keep the same proportion between classes as the original dataset
k = 2                   # The number of folds for the stratified k fold

interaction=True        # Whether we want to compute the interaction
drop=True               # Whether we want to drop the original features

model_f = models.logistic_regression    # The model we want to use


if __name__ == "__main__":

    # first, we load the dataset
    X, y = load_dataset('./data/data.pkl')
    
    if interaction:
        X = interactions(X, drop)

    # then, we split it
    X_tr, y_tr, X_te, y_te = split_dataset(X, y, test=test_size, stratify=stratify)
    # get the grid and the model
    model, grid = model_f()
    # call grid_search
    out = grid_search(X_tr, y_tr, model, grid, k)

